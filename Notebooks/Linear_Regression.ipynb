{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"light blue\">Simple Linear Regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">What is Simple Linear Regression? </font>\n",
    "\n",
    "Simple Linear Regression is a statistical method that helps us understand the relationship between two continuous variables. It's called \"simple\" because we're only dealing with two variables, and \"linear\" because we're looking for a straight-line relationship between them.\n",
    "\n",
    "##### **<font color=\"orange\">Example </font>**\n",
    "\n",
    "Let's say we want to predict the price of a house based on its size. We have data on the size of several houses (in square feet) and their corresponding prices. We can use Simple Linear Regression to find the relationship between these two variables.\n",
    "\n",
    "#### <font color=\"orange\">Mathematical Equation </font>\n",
    "\n",
    "The Simple Linear Regression equation is:\n",
    "\n",
    "Price(y) = β0 + β1 \\* Size(x) + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "* **Price** is the predicted price of the house\n",
    "* **Size** is the size of the house (in square feet)\n",
    "* **β0** is the intercept or constant term\n",
    "* **β1** is the slope coefficient - unit movement in the x-axis what is the unit movement in the y-axis.\n",
    "* **ε** is the error term (which represents the random variation in the data)\n",
    "\n",
    "#### How it Works\n",
    "\n",
    "Here's a step-by-step explanation:\n",
    "\n",
    "1. **Collect data**: We collect data on the size and price of several houses.\n",
    "2. **Plot the data**: We plot the data on a scatter plot to visualize the relationship between size and price.\n",
    "3. **Find the best-fit line**: We use a mathematical algorithm to find the best-fit line that minimizes the error between the predicted prices and the actual prices.\n",
    "4. **Interpret the results**: We interpret the results by looking at the slope coefficient (β1) and the intercept (β0). The slope coefficient tells us the change in price for a one-unit change in size. The intercept tells us the predicted price when the size is zero.\n",
    "\n",
    "#### <font color=\"orange\">Simple Linear Regression Plot </font>\n",
    "<img src=\"../Images/SimpleLinearRegression.jpg\" width=\"600\" height=\"400\">\n",
    "\n",
    "#### <font color=\"orange\">Cost Function </font>\n",
    "\n",
    "A cost function, also known as a loss function or objective function, is a mathematical function that measures the difference between the predicted output of a machine learning model and the actual true output.\n",
    "\n",
    "The cost function is typically denoted as J(θ), where θ represents the model's parameters. The cost function takes the following form:\n",
    "The formula for the cost function is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "* $J(\\theta)$: Cost function\n",
    "* $\\theta$: Model parameters\n",
    "* $m$: Number of training examples\n",
    "* $x^{(i)}$: $i^{th}$ input feature\n",
    "* $y^{(i)}$: $i^{th}$ actual true output\n",
    "* $h_\\theta(x^{(i)})$: Predicted output of the model for the $i^{th}$ datapoint\n",
    "\n",
    "\n",
    "<font color = 'red'>**Our main aim is to minimize the cost function**</font>\n",
    "\n",
    "\n",
    "#### <font color=\"orange\">Common Regression Metrics (Cost Function) </font>\n",
    "\n",
    "##### 1. Mean Absolute Error (MAE)\n",
    "The Mean Absolute Error measures the average magnitude of the errors in a set of predictions, without considering their direction. It gives an idea of how much the predicted values deviate from the actual values.\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "\n",
    "##### 2. Mean Squared Error (MSE)\n",
    "The Mean Squared Error is the average of the squared differences between the actual and predicted values. It penalizes larger errors more than smaller ones, making it sensitive to outliers.\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "##### 3. Root Mean Squared Error (RMSE)\n",
    "The Root Mean Squared Error is the square root of the Mean Squared Error. It provides an estimate of the standard deviation of the errors, making it easier to interpret in the same units as the target variable.\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "##### 4. Coefficient of Determination (R²)\n",
    "The R² score indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1, where 1 indicates perfect predictions.\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "\n",
    "##### 5. Adjusted R²\n",
    "The Adjusted R² adjusts the R² value to account for the number of predictors in the model. It prevents overestimating the model's explanatory power when additional predictors are included.\n",
    "\n",
    "$$\n",
    "R_{adj}^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "##### <font color=\"orange\">Example: Gradient Descent </font>\n",
    "\n",
    "Gradient Descent is a popular convergence algorithm used in machine learning. Here's how it works:\n",
    "- Initialize: Start with an initial guess for the parameters, θ.\n",
    "- Iterate: Use the current parameters to make predictions and calculate the cost, J(θ).\n",
    "- Improve: Update the parameters using the gradient of the cost function, ∇J(θ).\n",
    "- Repeat: Go back to step 2 and repeat until convergence.\n",
    "\n",
    "#### <font color=\"orange\">Convergence Algorithm </font>\n",
    "\n",
    "The convergence algorithm ensures that an iterative optimization process converges to a solution by repeatedly updating parameters until a stopping criterion is met.\n",
    "\n",
    "#### <font color=\"orange\">Convergence Algorithm Steps </font>\n",
    "\n",
    "1. **Initialize Parameters**:\n",
    "   Start with initial values for the parameters:\n",
    "    $ \\theta $, such as (  $ \\theta_1 $,  $ \\theta_2 $, $ \\ldots $, $ \\theta_n $).\n",
    "    \n",
    "\n",
    "2. **Compute the Objective Function**:\n",
    "   Evaluate the cost or loss function $ J(\\theta) $, which measures the difference between the predicted and actual values.\n",
    "\n",
    "   Example of a cost function (Mean Squared Error):\n",
    "   $$\n",
    "   J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "   $$\n",
    "\n",
    "3. **Compute the Gradient**:\n",
    "   Calculate the gradient of the cost function with respect to the parameters:\n",
    "   $$\n",
    "   \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "   $$\n",
    "\n",
    "   The gradient provides the direction and magnitude of the parameter updates.\n",
    "\n",
    "4. **Update the Parameters**:\n",
    "   Update the parameters using the chosen learning rate $ \\alpha $:\n",
    "   $$\n",
    "   \\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "   $$\n",
    "\n",
    "5. **Check the Stopping Criterion**:\n",
    "   Stop the iterations when one of the following conditions is met:\n",
    "   - The change in $ J(\\theta) $ between iterations is less than a predefined threshold $ \\epsilon $.\n",
    "     $$\n",
    "     |J(\\theta^{(t)}) - J(\\theta^{(t-1)})| < \\epsilon\n",
    "     $$\n",
    "   - The number of iterations exceeds the maximum allowed iterations.\n",
    "\n",
    "6. **Output the Optimal Parameters**:\n",
    "   Once convergence is achieved, return the optimal parameters $ \\theta^* $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
